#!/usr/bin/env python3
"""
æœ¬åœ°ç¿»è¯‘æ¨¡å‹ä½¿ç”¨ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•åœ¨å¤šè¯­è¨€è®ºå›ä¸­ä½¿ç”¨æœ¬åœ°ç¿»è¯‘æ¨¡å‹
"""

import asyncio
import os
import sys

async def test_local_translation_api():
    """æµ‹è¯•æœ¬åœ°ç¿»è¯‘APIæ¥å£"""
    
    print("ğŸŒ Local Translation Model Example")
    print("=" * 50)
    
    try:
        import httpx
        
        # å‡è®¾æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ
        base_url = "http://localhost:3001"
        
        # æµ‹è¯•ä¸åŒç±»å‹çš„ç¿»è¯‘è¯·æ±‚
        test_cases = [
            {
                "text": "Hello, this is a test of local translation!",
                "target_lang": "zh",
                "source_lang": "en",
                "service": "local",
                "description": "è‹±æ–‡åˆ°ä¸­æ–‡"
            },
            {
                "text": "ä½ å¥½ï¼Œè¿™æ˜¯æœ¬åœ°ç¿»è¯‘çš„æµ‹è¯•ï¼",
                "target_lang": "en", 
                "source_lang": "zh",
                "service": "local",
                "description": "ä¸­æ–‡åˆ°è‹±æ–‡"
            },
            {
                "text": "Bonjour, comment allez-vous?",
                "target_lang": "en",
                "source_lang": "fr",
                "service": "local",
                "description": "æ³•æ–‡åˆ°è‹±æ–‡"
            }
        ]
        
        async with httpx.AsyncClient() as client:
            print(f"ğŸ“¡ Testing API endpoint: {base_url}/api/translate/")
            
            for i, test_case in enumerate(test_cases, 1):
                print(f"\nğŸ§ª Test {i}: {test_case['description']}")
                print(f"ğŸ“ Original: {test_case['text']}")
                
                try:
                    response = await client.post(
                        f"{base_url}/api/translate/",
                        json={
                            "text": test_case["text"],
                            "target_lang": test_case["target_lang"],
                            "source_lang": test_case["source_lang"],
                            "service": test_case["service"]
                        },
                        timeout=30.0
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        print(f"âœ… Translated: {result['translated_text']}")
                        print(f"ğŸ”§ Service: {result['service']}")
                        print(f"ğŸ” Detected: {result.get('detected_language', 'N/A')}")
                    else:
                        print(f"âŒ Error: {response.status_code} - {response.text}")
                        
                except httpx.ConnectError:
                    print("âŒ Server not running. Start with: cd server && python run.py")
                    break
                except Exception as e:
                    print(f"âŒ Request failed: {e}")
    
    except ImportError:
        print("âŒ httpx not installed. Install with: pip install httpx")

def show_configuration_examples():
    """å±•ç¤ºä¸åŒé…ç½®é€‰é¡¹çš„ç¤ºä¾‹"""
    
    print("\nğŸ”§ Configuration Examples")
    print("=" * 50)
    
    configs = [
        {
            "name": "Hugging Face Transformers (Helsinki-NLP)",
            "description": "è½»é‡çº§ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒ",
            "env": {
                "LOCAL_MODEL_TYPE": "transformers",
                "LOCAL_MODEL_NAME": "helsinki-nlp/opus-mt-en-zh"
            }
        },
        {
            "name": "Hugging Face Transformers (M2M100)",
            "description": "å¤šè¯­è¨€æ”¯æŒï¼Œè´¨é‡æ›´é«˜",
            "env": {
                "LOCAL_MODEL_TYPE": "transformers", 
                "LOCAL_MODEL_NAME": "facebook/m2m100_418M"
            }
        },
        {
            "name": "Ollama (æœ¬åœ°å¤§æ¨¡å‹)",
            "description": "æœ€é«˜è´¨é‡ï¼Œéœ€è¦GPU",
            "env": {
                "LOCAL_MODEL_TYPE": "ollama",
                "LOCAL_MODEL_NAME": "llama2:7b",
                "OLLAMA_SERVER_URL": "http://localhost:11434"
            }
        },
        {
            "name": "è‡ªå®šä¹‰æ¨¡å‹æœåŠ¡å™¨",
            "description": "è‡ªå®šä¹‰æ¨¡å‹éƒ¨ç½²",
            "env": {
                "LOCAL_MODEL_TYPE": "custom",
                "LOCAL_MODEL_SERVER_URL": "http://localhost:8080",
                "CUSTOM_MODEL_PATH": "/path/to/your/model"
            }
        }
    ]
    
    for config in configs:
        print(f"\nğŸ“‹ {config['name']}")
        print(f"   {config['description']}")
        print("   ç¯å¢ƒå˜é‡é…ç½®:")
        for key, value in config['env'].items():
            print(f"   {key}={value}")

def show_installation_guide():
    """æ˜¾ç¤ºå®‰è£…æŒ‡å—"""
    
    print("\nğŸ“¦ Installation Guide")
    print("=" * 50)
    
    print("1ï¸âƒ£ åŸºç¡€ä¾èµ– (å¿…éœ€):")
    print("   pip install fastapi uvicorn httpx")
    
    print("\n2ï¸âƒ£ æœ¬åœ°æ¨¡å‹ä¾èµ– (å¯é€‰):")
    print("   # Hugging Face Transformers")
    print("   pip install torch transformers sentencepiece")
    print("   ")
    print("   # GPUæ”¯æŒ (æ¨è)")
    print("   pip install torch --index-url https://download.pytorch.org/whl/cu118")
    
    print("\n3ï¸âƒ£ Ollamaå®‰è£… (å¯é€‰):")
    print("   # macOS")
    print("   brew install ollama")
    print("   ")
    print("   # Linux")
    print("   curl -fsSL https://ollama.ai/install.sh | sh")
    print("   ")
    print("   # ä¸‹è½½æ¨¡å‹")
    print("   ollama pull llama2:7b")

def show_performance_comparison():
    """æ˜¾ç¤ºæ€§èƒ½å¯¹æ¯”"""
    
    print("\nâš¡ Performance Comparison")
    print("=" * 50)
    
    comparison = [
        ["Model Type", "Quality", "Speed", "Memory", "Use Case"],
        ["-" * 20, "-" * 10, "-" * 10, "-" * 10, "-" * 20],
        ["Helsinki-NLP", "ä¸­ç­‰", "å¿«", "ä½", "ç”Ÿäº§ç¯å¢ƒï¼Œç‰¹å®šè¯­è¨€å¯¹"],
        ["M2M100-418M", "é«˜", "ä¸­ç­‰", "ä¸­ç­‰", "å¤šè¯­è¨€æ”¯æŒ"],
        ["M2M100-1.2B", "å¾ˆé«˜", "æ…¢", "é«˜", "é«˜è´¨é‡ç¿»è¯‘"],
        ["Ollama LLM", "æœ€é«˜", "å¾ˆæ…¢", "å¾ˆé«˜", "æœ€é«˜è´¨é‡ï¼Œæœ‰GPU"],
        ["OpenAI API", "å¾ˆé«˜", "ä¸­ç­‰", "æ— ", "äº‘ç«¯æœåŠ¡ï¼ŒæŒ‰é‡è®¡è´¹"],
        ["DeepL API", "æœ€é«˜", "å¿«", "æ— ", "æ¬§æ´²è¯­è¨€ï¼ŒæŒ‰é‡è®¡è´¹"],
    ]
    
    for row in comparison:
        print(f"   {row[0]:<20} {row[1]:<10} {row[2]:<10} {row[3]:<10} {row[4]:<20}")

async def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸ¤– Multi-lingual Forum - Local Translation Example")
    print("ğŸŒ Breaking language barriers with AI")
    print("=" * 60)
    
    # æ˜¾ç¤ºé…ç½®ç¤ºä¾‹
    show_configuration_examples()
    
    # æ˜¾ç¤ºå®‰è£…æŒ‡å—
    show_installation_guide()
    
    # æ˜¾ç¤ºæ€§èƒ½å¯¹æ¯”
    show_performance_comparison()
    
    # æµ‹è¯•API (å¦‚æœæœåŠ¡å™¨è¿è¡Œä¸­)
    print("\nğŸ§ª Testing Local Translation API...")
    await test_local_translation_api()
    
    print("\n" + "=" * 60)
    print("ğŸ“š More information:")
    print("   - Local Models Guide: docs/local-models.md")
    print("   - Deployment Guide: docs/deployment.md")
    print("   - API Documentation: http://localhost:3001/docs")
    print("\nğŸš€ Happy translating!")

if __name__ == "__main__":
    asyncio.run(main()) 