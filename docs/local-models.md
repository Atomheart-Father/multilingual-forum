# ğŸ¤– æœ¬åœ°ç¿»è¯‘æ¨¡å‹éƒ¨ç½²æŒ‡å—

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•åœ¨å¤šè¯­è¨€è®ºå›ä¸­é…ç½®å’Œä½¿ç”¨æœ¬åœ°ç¿»è¯‘æ¨¡å‹ï¼Œå®ç°ç¦»çº¿ç¿»è¯‘ã€éšç§ä¿æŠ¤å’Œæˆæœ¬æ§åˆ¶ã€‚

## ğŸ¯ æœ¬åœ°æ¨¡å‹çš„ä¼˜åŠ¿

- **ğŸ”’ éšç§ä¿æŠ¤**: æ•°æ®ä¸ç¦»å¼€æœ¬åœ°æœåŠ¡å™¨
- **ğŸ’° æˆæœ¬æ§åˆ¶**: é¿å…APIè°ƒç”¨è´¹ç”¨
- **âš¡ ç¦»çº¿ä½¿ç”¨**: æ— éœ€ç½‘ç»œè¿æ¥
- **ğŸ›ï¸ å®Œå…¨æ§åˆ¶**: è‡ªå®šä¹‰æ¨¡å‹å’Œå‚æ•°

## ğŸ”§ æ”¯æŒçš„æ¨¡å‹ç±»å‹

### 1. Hugging Face Transformersæ¨¡å‹

æœ€ç®€å•çš„å…¥é—¨é€‰æ‹©ï¼Œæ”¯æŒå¤šç§é¢„è®­ç»ƒç¿»è¯‘æ¨¡å‹ã€‚

#### å®‰è£…ä¾èµ–
```bash
pip install torch transformers sentencepiece sacremoses
```

#### æ¨èæ¨¡å‹

**é€šç”¨ç¿»è¯‘æ¨¡å‹ï¼š**
- `facebook/m2m100_418M` - æ”¯æŒ100ç§è¯­è¨€çš„è½»é‡æ¨¡å‹
- `facebook/m2m100_1.2B` - æ›´é«˜è´¨é‡çš„å¤§å‹æ¨¡å‹

**ç‰¹å®šè¯­è¨€å¯¹æ¨¡å‹ï¼ˆHelsinki-NLPï¼‰ï¼š**
- `helsinki-nlp/opus-mt-en-zh` - è‹±æ–‡åˆ°ä¸­æ–‡
- `helsinki-nlp/opus-mt-zh-en` - ä¸­æ–‡åˆ°è‹±æ–‡
- `helsinki-nlp/opus-mt-en-de` - è‹±æ–‡åˆ°å¾·æ–‡
- `helsinki-nlp/opus-mt-fr-en` - æ³•æ–‡åˆ°è‹±æ–‡

#### é…ç½®ç¤ºä¾‹
```env
LOCAL_MODEL_TYPE=transformers
LOCAL_MODEL_NAME=facebook/m2m100_418M
```

### 2. Ollamaæœ¬åœ°å¤§è¯­è¨€æ¨¡å‹

ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œç¿»è¯‘ï¼Œè´¨é‡æ›´é«˜ä½†èµ„æºæ¶ˆè€—æ›´å¤§ã€‚

#### å®‰è£…Ollama
```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# å¯åŠ¨æœåŠ¡
ollama serve
```

#### ä¸‹è½½æ¨¡å‹
```bash
# ä¸‹è½½å¤šè¯­è¨€æ¨¡å‹
ollama pull llama2:7b
ollama pull codellama:7b
ollama pull mistral:7b

# ä¸‹è½½ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹
ollama pull qwen:7b
ollama pull chatglm3:6b
```

#### é…ç½®ç¤ºä¾‹
```env
LOCAL_MODEL_TYPE=ollama
LOCAL_MODEL_NAME=llama2:7b
OLLAMA_SERVER_URL=http://localhost:11434
```

### 3. è‡ªå®šä¹‰æ¨¡å‹æœåŠ¡å™¨

éƒ¨ç½²è‡ªå·±çš„ç¿»è¯‘æ¨¡å‹æœåŠ¡å™¨ã€‚

#### é…ç½®ç¤ºä¾‹
```env
LOCAL_MODEL_TYPE=custom
LOCAL_MODEL_SERVER_URL=http://localhost:8080
CUSTOM_MODEL_PATH=/path/to/your/model
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. é€‰æ‹©æ¨¡å‹ç±»å‹

æ ¹æ®æ‚¨çš„éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼š

| æ¨¡å‹ç±»å‹ | è´¨é‡ | é€Ÿåº¦ | èµ„æºéœ€æ±‚ | é€‚ç”¨åœºæ™¯ |
|---------|------|------|----------|----------|
| Helsinki-NLP | ä¸­ | å¿« | ä½ | ç‰¹å®šè¯­è¨€å¯¹ï¼Œç”Ÿäº§ç¯å¢ƒ |
| M2M100 | é«˜ | ä¸­ | ä¸­ | å¤šè¯­è¨€æ”¯æŒï¼Œä¸­ç­‰è§„æ¨¡ |
| Ollama (LLM) | å¾ˆé«˜ | æ…¢ | é«˜ | é«˜è´¨é‡ç¿»è¯‘ï¼Œæœ‰GPU |

### 2. å®‰è£…ä¾èµ–

```bash
# åŸºç¡€ä¾èµ–
pip install torch transformers

# å¦‚æœä½¿ç”¨GPU
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 3. é…ç½®ç¯å¢ƒå˜é‡

ç¼–è¾‘ `server/.env` æ–‡ä»¶ï¼š

```env
# å¯ç”¨æœ¬åœ°æ¨¡å‹
LOCAL_MODEL_TYPE=transformers
LOCAL_MODEL_NAME=facebook/m2m100_418M
```

### 4. æµ‹è¯•ç¿»è¯‘

```bash
curl -X POST http://localhost:3001/api/translate/ \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Hello, world!",
    "target_lang": "zh",
    "service": "local"
  }'
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. GPUåŠ é€Ÿ

```python
# æ£€æŸ¥GPUå¯ç”¨æ€§
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU count: {torch.cuda.device_count()}")
```

### 2. æ¨¡å‹ç¼“å­˜

æ¨¡å‹ä¼šè‡ªåŠ¨ç¼“å­˜åˆ°ï¼š
- Windows: `C:\Users\{username}\.cache\huggingface\transformers`
- macOS/Linux: `~/.cache/huggingface/transformers`

### 3. æ‰¹é‡ç¿»è¯‘

å¯¹äºå¤§é‡æ–‡æœ¬ï¼Œè€ƒè™‘å®ç°æ‰¹é‡ç¿»è¯‘æ¥å£ï¼š

```python
# æ‰¹é‡ç¿»è¯‘ç¤ºä¾‹
async def batch_translate(texts: List[str], target_lang: str):
    # å®ç°æ‰¹é‡å¤„ç†é€»è¾‘
    pass
```

## ğŸ› ï¸ é«˜çº§é…ç½®

### 1. è‡ªå®šä¹‰æ¨¡å‹æœåŠ¡å™¨

åˆ›å»ºç‹¬ç«‹çš„æ¨¡å‹æœåŠ¡å™¨ï¼š

```python
# model_server.py
from fastapi import FastAPI
from transformers import pipeline

app = FastAPI()
translator = pipeline("translation", model="your-model")

@app.post("/translate")
async def translate(request: dict):
    result = translator(request["text"])
    return {"translated_text": result[0]["translation_text"]}
```

### 2. æ¨¡å‹å¾®è°ƒ

é’ˆå¯¹ç‰¹å®šé¢†åŸŸå¾®è°ƒæ¨¡å‹ï¼š

```python
# ç¤ºä¾‹ï¼šé’ˆå¯¹æŠ€æœ¯æ–‡æ¡£å¾®è°ƒ
from transformers import AutoModelForSeq2SeqLM, Trainer

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = AutoModelForSeq2SeqLM.from_pretrained("helsinki-nlp/opus-mt-en-zh")

# å‡†å¤‡è®­ç»ƒæ•°æ®
# ...

# å¾®è°ƒè®­ç»ƒ
trainer = Trainer(model=model, train_dataset=train_dataset)
trainer.train()
```

### 3. å¤šæ¨¡å‹è´Ÿè½½å‡è¡¡

éƒ¨ç½²å¤šä¸ªæ¨¡å‹å®ä¾‹è¿›è¡Œè´Ÿè½½å‡è¡¡ï¼š

```yaml
# docker-compose.yml
services:
  model-server-1:
    image: your-model-server
    ports: ["8001:8000"]
  
  model-server-2:
    image: your-model-server
    ports: ["8002:8000"]
  
  nginx:
    image: nginx
    # é…ç½®è´Ÿè½½å‡è¡¡
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **å†…å­˜ä¸è¶³**
   ```bash
   # ä½¿ç”¨æ›´å°çš„æ¨¡å‹
   LOCAL_MODEL_NAME=helsinki-nlp/opus-mt-en-zh
   ```

2. **GPUä¸å¯ç”¨**
   ```bash
   # å®‰è£…CUDAç‰ˆæœ¬çš„PyTorch
   pip install torch --index-url https://download.pytorch.org/whl/cu118
   ```

3. **æ¨¡å‹ä¸‹è½½å¤±è´¥**
   ```bash
   # è®¾ç½®é•œåƒæº
   export HF_ENDPOINT=https://hf-mirror.com
   ```

### ç›‘æ§å’Œæ—¥å¿—

```python
# æ·»åŠ æ€§èƒ½ç›‘æ§
import time
import psutil

def monitor_translation_performance():
    start_time = time.time()
    memory_before = psutil.Process().memory_info().rss
    
    # æ‰§è¡Œç¿»è¯‘
    result = translate(text)
    
    end_time = time.time()
    memory_after = psutil.Process().memory_info().rss
    
    logger.info(f"Translation took {end_time - start_time:.2f}s")
    logger.info(f"Memory used: {(memory_after - memory_before) / 1024 / 1024:.2f}MB")
```

## ğŸ“š æ›´å¤šèµ„æº

- [Hugging Faceæ¨¡å‹åº“](https://huggingface.co/models?pipeline_tag=translation)
- [Ollamaæ¨¡å‹åº“](https://ollama.ai/library)
- [PyTorchå®˜æ–¹æ–‡æ¡£](https://pytorch.org/docs/)
- [Transformersæ–‡æ¡£](https://huggingface.co/docs/transformers)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤æ–°çš„æ¨¡å‹é›†æˆå’Œä¼˜åŒ–å»ºè®®ï¼ 